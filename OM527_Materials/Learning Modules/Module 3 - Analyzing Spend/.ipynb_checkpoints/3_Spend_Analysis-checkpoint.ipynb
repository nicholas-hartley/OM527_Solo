{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Spend Analysis\n",
    "Prepared by: Nickolas K. Freeman, PhD\n",
    "\n",
    "From Wikipedia:\n",
    "\n",
    "> Spend Analysis is the process of collecting, cleansing, classifying and analyzing expenditure data with the purpose of decreasing procurement costs, improving efficiency, and monitoring controls and compliance. It can also be leveraged in other areas of business such as inventory management, contract management, complex sourcing, supplier management, budgeting, planning, and product development.\n",
    ">\n",
    "> There are three core areas of spend analysis - visibility, analysis, and process. By leveraging all three, companies can generate answers to the crucial questions affecting their spending, including:\n",
    ">\n",
    "> - What am I really spending?\n",
    "> - With whom am I spending it?\n",
    "> - Am I getting what was promised for that spend?\n",
    "> Spend analysis is often viewed as part of a larger domain known as spend management which incorporates spend analysis, commodity management and strategic sourcing.\n",
    "> \n",
    "> Companies perform a spend analysis for several reasons. The core business driver for most organizations is profitability. In addition to improving compliance and reducing cycle times, performing detailed spend analysis helps companies find new areas of savings that previously went untapped, and hold on to past areas of savings that they have already negotiated.\n",
    "\n",
    "In this notebook, we will use U.S. government data on contract spending available from https://usaspending.org to analyze FY 2019 expenditures in the state of Alabama. In particular, we will investigate:\n",
    "- Which products and services are being purchased?\n",
    "- Who is purchasing these products or services?\n",
    "- From whom are the products or services being purchased?\n",
    "- How are these products or services being purchased? \n",
    "\n",
    "We will be using Python and the `pandas` library to demonstrate a simple spend analysis. The data we will be analyzing is the same that we used in the *Pandas Overview* notebook. The data, which was downloaded from https://usaspending.org, includes records of government expenditures by the state of Alabama durng fiscal year 2019 and is provided in a *comma-separated value* format. The following code block imports the necessary libraries, loads the data, and stores it in a variable named `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Specify maximum columns = 40\n",
    "pd.set_option('display.max_columns', 40)\n",
    "\n",
    "# Specify floating-point precision\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "data_filepath = pathlib.Path('data', 'AL_FY2019.csv')\n",
    "\n",
    "if data_filepath.exists():\n",
    "    print('File exists, reading with pandas.')\n",
    "    data = pd.read_csv(data_filepath)\n",
    "else:\n",
    "    print('File does not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which products/services are being purchased?\n",
    "\n",
    "We will now proceed to investigate the first question: Which products/services are being purchased?\n",
    "\n",
    "Let's begin by reminding ourselves of the available columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The North American Industry Classification System (NAICS) is the standard used by Federal agencies in classifying business establishments for the purpose of collecting, analyzing, and publishing data related to the U.S. business economy. We will use the NAICS codes provided as a proxy for the product/service type. \n",
    "\n",
    "We will use the groupby method of available to `pandas` `DataFrame` objects to determine the frequently purchased products/services. The following code block shows how we can use this functionality to group the data by `naics_code` and `naics_description`, counting the number of unique (`nunique`) awards for each group and summing the total dollars obligated for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_columns = ['naics_code', 'naics_description']\n",
    "\n",
    "agg_dict = {\n",
    "    'award_id_piid':['nunique'],\n",
    "    'total_dollars_obligated': ['sum'],\n",
    "}\n",
    "\n",
    "grouped_data = data.groupby(groupby_columns).agg(agg_dict)\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the object returned is a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grouped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have a multi-dimensional index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our column names are also stored as a multi-dimenional index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personally, I like to flatten the multi-dimensional column names using the following approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data.columns = ['_'.join(col).strip() for col in grouped_data.columns.values]\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the underlying dependence on `numpy`, calculations using the columns are easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data['dollars_obligated_proportion'] = (grouped_data['total_dollars_obligated_sum']\n",
    "                                               /grouped_data['total_dollars_obligated_sum'].sum())\n",
    "\n",
    "grouped_data['awards_proportion'] = (grouped_data['award_id_piid_nunique']/\n",
    "                                     grouped_data['award_id_piid_nunique'].sum())\n",
    "\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nlargest` method allows us to quickly identify the subset of data associated with top values in one or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data.nlargest(10, columns = ['dollars_obligated_proportion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how we can *chain* methods to save the top 10 NAICS values to a list. However, since we had a multidimensional index, our list is a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_naics_codes = grouped_data.nlargest(10, columns = ['dollars_obligated_proportion']).index.tolist()\n",
    "top_naics_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will answer the remaining questions with respect to the top NAICS code. However, instead of hard-coding the NAICS code, we will just index the first value in the `top_naics_codes` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_val = 0\n",
    "\n",
    "top_naics_codes[index_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this returns a `tuple`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(top_naics_codes[index_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the NAICS code by accessing the first element of the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_naics_codes[index_val][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the NAICS description by accessing the second element of the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_naics_codes[index_val][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block saves these values as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NAICS code and description\n",
    "naics_code = top_naics_codes[index_val][0]\n",
    "naics_description = top_naics_codes[index_val][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block saves the data associated with the current NAICS code as a variable named `naics_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to subset the data\n",
    "mask = data['naics_code'] == naics_code\n",
    "\n",
    "# Get the data subset\n",
    "naics_data = data[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who is purchasing these products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block determines the agencies with transactions associated with the target NAICS number and the total amount of money obligated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data\n",
    "groupby_columns = ['awarding_agency_abbr', 'awarding_agency_name']\n",
    "\n",
    "agg_dict = {\n",
    "    'award_id_piid': ['nunique'],\n",
    "    'total_dollars_obligated': ['sum'],\n",
    "}\n",
    "\n",
    "grouped_data = naics_data.groupby(groupby_columns).agg(agg_dict)\n",
    "\n",
    "grouped_data.columns = ['_'.join(col).strip() for col in grouped_data.columns.values]\n",
    "\n",
    "# Calculate the proportions\n",
    "grouped_data['dollar_obligated_proportion'] = (grouped_data['total_dollars_obligated_sum']\n",
    "                                               /grouped_data['total_dollars_obligated_sum'].sum())\n",
    "\n",
    "grouped_data['awards_proportion'] = (grouped_data['award_id_piid_nunique']\n",
    "                                     /grouped_data['award_id_piid_nunique'].sum())\n",
    "\n",
    "# Print the data sorted by the number of awards\n",
    "grouped_data.sort_values('awards_proportion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we essentially copied and pasted the grouping and column calculation code from earlier in the cell above. Anytime you notice yourself doing so, look into defining a function. This is done in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_grouper(df, agg_dict, groupby_columns):\n",
    "    '''\n",
    "    This function groups the provided DataFrame, df, by the columns\n",
    "    specified in the groupby_columns argument. The aggregations specified\n",
    "    in the agg_dict dictionary are applied. Also, each numeric column in the \n",
    "    aggregated DataFrame is used to create a proportion column. The aggregated data\n",
    "    is returned as a DataFrame sorted by the keys of the agg_dict\n",
    "    dictionary, in the order they are specified, i.e., first key\n",
    "    has a higher sort priority than the second, etc...\n",
    "    '''\n",
    "    \n",
    "    grouped_df = df.groupby(groupby_columns).agg(agg_dict)\n",
    "    \n",
    "    grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n",
    "    \n",
    "    numeric_columns = grouped_df.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        grouped_df[f'{column}_proportion'] = (grouped_df[column]/grouped_df[column].sum())\n",
    "        \n",
    "    grouped_df = grouped_df.sort_values(numeric_columns)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how we can use the newly defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_columns = ['awarding_agency_abbr', 'awarding_agency_name']\n",
    "\n",
    "agg_dict = {\n",
    "    'total_dollars_obligated': ['sum'], \n",
    "    'award_id_piid': ['nunique'],\n",
    "}\n",
    "\n",
    "custom_grouper(naics_data, agg_dict, groupby_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who are we purchasing from?\n",
    "\n",
    "The following code block determines the total amount and number of agencies assciated with transactions, grouped by the recipient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_columns = ['recipient_duns', 'recipient_name']\n",
    "\n",
    "agg_dict = {\n",
    "    'awarding_agency_abbr': ['nunique'],\n",
    "    'total_dollars_obligated': ['sum'],\n",
    "}\n",
    "\n",
    "custom_grouper(naics_data, agg_dict, groupby_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are we purchasing?\n",
    "\n",
    "The following code block determines the total amount associated with transactions, grouped by the contract pricing type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_columns = ['type_of_contract_pricing', \n",
    "               'type_of_contract_pricing_code']\n",
    "\n",
    "agg_dict = {\n",
    "    'total_dollars_obligated': ['sum'],\n",
    "}\n",
    "\n",
    "custom_grouper(naics_data, agg_dict, groupby_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block determines the total amount associated with transactions, grouped by the solicitation procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_columns = ['extent_competed', \n",
    "                   'solicitation_procedures']\n",
    "\n",
    "agg_dict = {\n",
    "    'total_dollars_obligated': ['sum'],\n",
    "}\n",
    "\n",
    "custom_grouper(naics_data, agg_dict, groupby_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take everything we have done so far and automate the analysis for the 10 NAICS codes with the highest total dollars obligated. The following code block redefines our `custom_grouper` function. This is not necessary, but done simply to get everything in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_grouper(df, agg_dict, groupby_columns):\n",
    "    '''\n",
    "    This function groups the provided DataFrame, df, by the columns\n",
    "    specified in the groupby_columns argument. The aggregations specified\n",
    "    in the agg_dict dictionary are applied. Also, each numeric column in the \n",
    "    aggregated DataFrame is used to create a proportion column. The aggregated data\n",
    "    is returned as a DataFrame sorted by the keys of the agg_dict\n",
    "    dictionary, in the order they are specified, i.e., first key\n",
    "    has a higher sort priority than the second, etc...\n",
    "    '''\n",
    "    \n",
    "    grouped_df = df.groupby(groupby_columns).agg(agg_dict)\n",
    "    \n",
    "    grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n",
    "    \n",
    "    numeric_columns = grouped_df.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        grouped_df[f'{column}_proportion'] = (grouped_df[column]/grouped_df[column].sum())\n",
    "        \n",
    "    grouped_df = grouped_df.sort_values(numeric_columns)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block specifies a simple function that we will use to print status messages during the automated analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status(message, \n",
    "                 start_timestamp, \n",
    "                 current_timestamp, \n",
    "                 width = 60):\n",
    "    '''\n",
    "    A helper function to print a status message with elapsed time in seconds.\n",
    "    '''\n",
    "    \n",
    "    print(f'{message} (elapsed time: {round(current_timestamp - start_timestamp, 2)} seconds)')\n",
    "    print('-'*width)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block specifies the automated analysis. For each of the top 10 NAICS codes, an Excel file is written in the `outputs` folder with tabs for the various analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cell = True\n",
    "\n",
    "if run_cell:\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    output_folder_path = pathlib.Path('outputs')\n",
    "    if not output_folder_path.exists():\n",
    "        output_folder_path.mkdir(exist_ok = True)\n",
    "    \n",
    "    print_status(message = 'Reading data', \n",
    "                 start_timestamp = start_time, \n",
    "                 current_timestamp = time.time())\n",
    "    \n",
    "    data_filepath = pathlib.Path('data', 'AL_FY2019.csv')\n",
    "    if data_filepath.exists():\n",
    "        print('File exists, reading with pandas.')\n",
    "        data = pd.read_csv(data_filepath)\n",
    "    else:\n",
    "        print('File does not exist!')\n",
    "        \n",
    "    print_status(message = 'Dropping non-positive values for total_dollars_obligated', \n",
    "                 start_timestamp = start_time, \n",
    "                 current_timestamp = time.time())\n",
    "        \n",
    "    mask = data['total_dollars_obligated'] <= 0\n",
    "    data = data[~mask]\n",
    "\n",
    "    \n",
    "    print_status(message = 'Determining top NAICS codes by total_dollars_obligated', \n",
    "                 start_timestamp = start_time, \n",
    "                 current_timestamp = time.time())\n",
    "    \n",
    "    groupby_columns = ['naics_code', \n",
    "                       'naics_description']\n",
    "    agg_dict = {\n",
    "        'award_id_piid': ['nunique'], \n",
    "        'total_dollars_obligated': ['sum'],\n",
    "    }\n",
    "\n",
    "    grouped_data = custom_grouper(data, agg_dict, groupby_columns)\n",
    "\n",
    "    top_naics_codes = grouped_data.nlargest(10, columns = ['total_dollars_obligated_sum']).index.tolist()\n",
    "\n",
    "    for naics_code, naics_description in top_naics_codes:\n",
    "        \n",
    "        print_status(message = f'Starting analysis for NAICS code {int(naics_code)}', \n",
    "                 start_timestamp = start_time, \n",
    "                 current_timestamp = time.time())\n",
    "        \n",
    "        output_filepath = pathlib.Path('outputs', f'{int(naics_code)}_output.xlsx')\n",
    "        with pd.ExcelWriter(output_filepath) as writer:  \n",
    "            mask = data['naics_code'] == naics_code\n",
    "            naics_data = data[mask]\n",
    "\n",
    "            # Who is buying\n",
    "            groupby_columns = ['awarding_agency_abbr', \n",
    "                               'awarding_agency_name']\n",
    "            agg_dict = {\n",
    "                'total_dollars_obligated': ['sum'],\n",
    "                'award_id_piid': ['nunique'],\n",
    "            }            \n",
    "            custom_grouper(naics_data, agg_dict, groupby_columns).to_excel(writer, 'Buying Agencies')\n",
    "\n",
    "\n",
    "            # Who are we buying from\n",
    "            groupby_columns = ['recipient_duns', \n",
    "                               'recipient_name']\n",
    "            agg_dict = {\n",
    "                'awarding_agency_abbr': ['nunique'],\n",
    "                'total_dollars_obligated': ['sum'],\n",
    "            }\n",
    "            custom_grouper(naics_data, agg_dict, groupby_columns).to_excel(writer, 'Recipients')\n",
    "\n",
    "            \n",
    "            # how are we buying - contracts\n",
    "            groupby_columns = ['type_of_contract_pricing', \n",
    "                               'type_of_contract_pricing_code']\n",
    "            agg_dict = {\n",
    "                'total_dollars_obligated': ['sum'],\n",
    "            }            \n",
    "            custom_grouper(naics_data, agg_dict, groupby_columns).to_excel(writer, 'Contract Types')\n",
    "\n",
    "            \n",
    "            # how are we buying - solicitation\n",
    "            groupby_columns = ['extent_competed', \n",
    "                               'solicitation_procedures']\n",
    "            agg_dict = {\n",
    "                'total_dollars_obligated': ['sum'],\n",
    "            }\n",
    "            custom_grouper(naics_data, agg_dict, groupby_columns).to_excel(writer, 'Solicitation Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
