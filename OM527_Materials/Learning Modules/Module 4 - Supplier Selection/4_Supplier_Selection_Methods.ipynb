{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Supplier Ranking Methods (Multi-criteria Decision-making or MCDM)\n",
    "Prepared by: Nickolas Freeman, Ph.D.\n",
    "\n",
    "This notebook presents techniques for ranking suppliers. These techniques are useful for identifying a subset of candidate suppliers from a potentially large set of available suppliers. The difficulty in such selection tasks is that oftentimes there are several competing criteria that we would like to use for evaluation. For example, buying firms care about both cost and quality. However, it may be impossible to find a supplier that offers exceptional performance with respect to both criteria because the two criteria may be negatively correlated (e.g., the supplier is able to offer products at a lower cost because they do not enforce very stringent quality controls). \n",
    "\n",
    "Another complicating factor is that it is common for a buying firm to be interested in both quantitative and qualitative criteria. However, comparing scores for quantitative and qualitative aspects is difficult. For example, how much of a cost increase is improved technical support or sustainability efforts worth? The approaches we consider in this notebook assign a quantitative score to subjective criteria and employ relatively simple weighting techniques to determine overall scores. A hyperlinked table of contents follows.\n",
    "\n",
    "<a id=\"Table_of_Contents\"> </a>\n",
    "# Table of Contents\n",
    "1. [Data Preparation](#Data_Preparation)<br>\n",
    "2. [Technique 1: Weighted Sum Method](#Weighted_Sum_Method)<br>\n",
    "3. [Technique 2: Weighted Product Method](#Weighted_Product_Method)<br>\n",
    "4. [The OM527_functions package and the mcdm Module](#mcdm_functions)<br>\n",
    "5. [Weight Determination via Analytic Hierarchy Process (AHP)](#AHP)<br>\n",
    "6. [Ensemble Approaches](#Ensemble)<br>\n",
    "    \n",
    "The following code block imports some packages that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "from ipywidgets import interact, interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', '{:,.3f}'.format)\n",
    "import seaborn as sns\n",
    "sns.set_style(style = 'whitegrid')\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the aggregation capabilities provided by the `pandas` library at some points in our analysis. Thus, the following code block defines the `custom_grouper` function that we developed when looking at the concept of spend analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_grouper(df, agg_dict, groupby_columns):\n",
    "    '''\n",
    "    This function groups the provided DataFrame, df, by the columns\n",
    "    specified in the groupby_columns argument. The aggregations specified\n",
    "    in the agg_dict dictionary are applied. Also, each numeric column in the \n",
    "    aggregated DataFrame is used to create a proportion column. The aggregated data\n",
    "    is returned as a DataFrame sorted by the keys of the agg_dict\n",
    "    dictionary, in the order they are specified, i.e., first key\n",
    "    has a higher sort priority than the second, etc...\n",
    "    '''\n",
    "    \n",
    "    grouped_df = df.groupby(groupby_columns).agg(agg_dict)\n",
    "    \n",
    "    grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n",
    "    \n",
    "    numeric_columns = grouped_df.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        grouped_df[f'{column}_proportion'] = (grouped_df[column]/grouped_df[column].sum())\n",
    "        \n",
    "    grouped_df = grouped_df.sort_values(numeric_columns)\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook, we will use a dataset that includes data for several trucking service suppliers. The data includes qualitative and quantitative factors that we will use to rank the suppliers. The following code block 1) reads the data into a Pandas DataFrame named `tr_data`, 2) prints the number of rows and columns using the `shape` of the DataFrame, and 3) prints the first five rows of the data using the `head` method of the DataFrame. \n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path('data', 'tr_supplier_data.csv')\n",
    "tr_data = pd.read_csv(data_path)\n",
    "\n",
    "rows, columns = tr_data.shape\n",
    "print(f'The data has {rows} rows and {columns} columns.')\n",
    "\n",
    "tr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Data_Preparation\"> </a>\n",
    "# Data Preparation\n",
    "\n",
    "A key step in any analysis is data preparation. Thus, instead of brushing over this aspect an giving you a *clean* dataset, we will go through the process of preparing our data for analysis. We will assume that we want to use the following criteria to rank the suppliers:\n",
    "\n",
    "1. we only want to consider companies that are primarily focused on specialized long distance trucking,\n",
    "2. whether the supplier is open on Saturdays (yes is better),\n",
    "3. whether the supplier is open on Sundays (yes is better),\n",
    "4. the suppliers credit score (higher is better),\n",
    "5. the size of the supplier (higher is better),\n",
    "6. the suppliers location (Tuscaloosa is preferred, AL is second preference)\n",
    "\n",
    "The following code block allows us to inspect the primary NAICS for all entries in the data. Note that the description for NAICS code 484230 most closely matches what we desire.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_columns = ['Primary NAICS', \n",
    "                   'Primary NAICS Description']\n",
    "agg_dict = {\n",
    "    'State': ['count'],\n",
    "}\n",
    "\n",
    "custom_grouper(tr_data, agg_dict, groupby_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block restricts the data to only include NAICS code 484230.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_naics = [484230]\n",
    "\n",
    "tr_data = tr_data[tr_data['Primary NAICS'].isin(relevant_naics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider whether the suppliers are open on Saturday or Sunday. The following code block prints the unique values in the `Saturday Open` and `Sunday Open` columns.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The unique values in the Saturday Open column are {tr_data[\"Saturday Open\"].unique()}')\n",
    "print(f'The unique values in the Sunday Open column are {tr_data[\"Sunday Open\"].unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we cannot determine whether a supplier with a `nan` value in the columns is open or not, we will interpret the values `Closed` and `nan` as indicating that a supplier is not open on a given day. The following code block uses this decision rule to assign a score of 1 to suppliers that are open on Saturday(Sunday) and a score of 0.1 to suppliers that are not open on Saturday(Sunday).\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonopen_values = [np.nan, 'Closed']\n",
    "\n",
    "saturday_closed_mask = tr_data['Saturday Open'].isin(nonopen_values)\n",
    "tr_data['Open Saturday'] = 1\n",
    "tr_data.loc[saturday_closed_mask, 'Open Saturday'] = 0.1\n",
    "\n",
    "sunday_closed_mask = tr_data['Sunday Open'].isin(nonopen_values)\n",
    "tr_data['Open Sunday'] = 1\n",
    "tr_data.loc[sunday_closed_mask, 'Open Sunday'] = 0.1\n",
    "\n",
    "tr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider the credit score for each supplier. The following code block prints the unique values that occur in the `Credit Score Alpha` column.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['Credit Score Alpha'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a dictionary object that maps each of the observed credit score values to a value in the interval $[0,1]$, where higher values are better.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_score_mapper = {\n",
    "    'A+': 1,\n",
    "    'A': 0.95,\n",
    "    'B+': 0.7,\n",
    "    'B': 0.5,\n",
    "    'C': 0,\n",
    "    'C+': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block use the `map` method to lookup values in the `Credit Score Alpha` column in the `credit_score_mapper` object and assign the appropriate values back to the DataFrame.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['Credit Score Num'] = tr_data['Credit Score Alpha'].map(credit_score_mapper)\n",
    "tr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now consider the size of the supplier's business. We asume that we are interested in working with larger, presumably more established companies. If you look at the columns, there are several columns that we might be able to use as a proxy for size. In particular, we might consider `Location Employee Size Actual`, `Location Sales Volume Actual`, and maybe even `Square Footage`. Looking at the data, one will observe that there is a high degree of correlation among the values in these columns. We will use `Location Sales Volume Actual`as a proxy for the size of the supplier's buisness. The following code block prints the column. Note that the reported `dtype` is `object`. This is telling is that pandas is interpreting the values in the column as strings.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['Location Sales Volume Actual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the `Location Sales Volume Actual` column are being interpreted as strings because of the `$` signs and commas. The following code block shows how we can use *string methods* on a pandas column to remove these characters.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['Location Sales Volume Actual'] = tr_data['Location Sales Volume Actual'].str.replace('$', '')\n",
    "tr_data['Location Sales Volume Actual'] = tr_data['Location Sales Volume Actual'].str.replace(',', '')\n",
    "tr_data['Location Sales Volume Actual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block converts the type of data stored in the `Location Sales Volume Actual` column to a numeric type.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['Location Sales Volume Actual'] = pd.to_numeric(tr_data['Location Sales Volume Actual'])\n",
    "tr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing we wanted to consider was the supplier's location. Specifically, we want to enforce a rating that assigns the highest weight to suppliers in Tuscaloosa, the second highest weight to suppliers in Alabama, and a base weight to all other suppliers. To apply such a rule, we will construct a dictionary of zip codes that contains the appropriate values. We will then map this dictionary back onto the data as we did when considering supplier credit scores. The first step in doing this is to determine all of the unique (city, state, zip code) tuples in the data. This is accomplished in the following code block. The first five tuples are printed.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_columns = ['City', 'State', 'ZIP Code']\n",
    "agg_dict = {'Company Name': 'count'}\n",
    "\n",
    "city_state_pairs = tr_data.groupby(agg_columns).agg(agg_dict).index.tolist()\n",
    "city_state_pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block uses basic flow control to create the dictionary that will be used for mapping. Some values are printed.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_code_mapper = {}\n",
    "for city, state, zip_code in city_state_pairs:\n",
    "    if ((city == 'Tuscaloosa') and (state == 'AL')):\n",
    "        zip_code_mapper[zip_code] = 1\n",
    "    elif state == 'AL':\n",
    "        zip_code_mapper[zip_code] = 0.8\n",
    "    else:\n",
    "        zip_code_mapper[zip_code] = 0.5\n",
    "        \n",
    "print(f'The value for 35405 is {zip_code_mapper[35405]}')\n",
    "print(f'The value for 37705 is {zip_code_mapper[37705]}')\n",
    "print(f'The value for 35210 is {zip_code_mapper[35210]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block maps the values onto the data and prints the first five rows.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['Zip Code Score'] = tr_data['ZIP Code'].map(zip_code_mapper)\n",
    "tr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, notice that some companies seem to operate multiple locations. This can be seen by getting the number of unique zip codes for each company as shown in the following code block. \n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_columns = ['Company Name']\n",
    "\n",
    "agg_dict = {\n",
    "    'ZIP Code': ['unique', 'nunique'],\n",
    "}\n",
    "\n",
    "custom_grouper(tr_data, agg_dict, groupby_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want one observation for each company in our final data. Thus, **let's assume that we chose to handle duplicate companies by averaging the values in the `Open Saturday`, `Open Sunday`, `Credit Score Num`, and `Zip Code Score` columns. We will sum the values in the `Location Sales Volume Actual Column`.** These aggregations are achieved in the following code block. Note that I do not use the `custom_grouper` function because I do not need the proportion columns. Also, I want to retain the original column names, so I just use the keys of the `agg_dict` object to rename the columns. The `DataFrame` returned by the `groupby` application will only contain the columns we need for our analysis. Thus, we store the `DataFrame` in a new variable named `clean_data`.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_columns = ['Company Name']\n",
    "\n",
    "agg_dict = {\n",
    "    'Location Sales Volume Actual': ['sum'],\n",
    "    'Open Saturday': ['mean'],\n",
    "    'Open Sunday': ['mean'],\n",
    "    'Credit Score Num': ['mean'],\n",
    "    'Zip Code Score': ['mean'],\n",
    "}\n",
    "\n",
    "clean_data = tr_data.groupby(groupby_columns).agg(agg_dict)\n",
    "clean_data.columns = agg_dict.keys()\n",
    "clean_data = clean_data.reset_index()\n",
    "clean_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the values in the `Location Sales Volume Actual` column are significantly larger than those in the `Open Saturday`, `Open Sunday`, `Credit Score Num`, and `Zip Code Score` columns. Depending on the ranking method we use, this could cause the rankings to be disproportionately biased by the sales volume. In general, it is good to normalize all values that we will be using for the ranking so that they range from zero to one. The following code block shows how we can use the `MinMaxScaler` from the `sklearn.preprocessing` module to rescale the values.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sales volume data as a numpy array\n",
    "sales_volume_array = clean_data['Location Sales Volume Actual'].values\n",
    "\n",
    "# Reshape the array so that is multi-dimensional with one column\n",
    "# and as many rows as needed\n",
    "sales_volume_array = sales_volume_array.reshape(-1, 1)\n",
    "\n",
    "# Normalize the array\n",
    "clean_data['Location Sales Volume Actual'] = preprocessing.MinMaxScaler().fit_transform(sales_volume_array)\n",
    "\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data prepared, we will now define the weights that we will use for supplier ranking. These weights indicate the relative importance of the various attributes for each supplier. We will define these weights using the dictionary that is specified in the following code block.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'Open Saturday': 3,\n",
    "    'Open Sunday': 2, \n",
    "    'Credit Score Num': 9, \n",
    "    'Zip Code Score': 7,\n",
    "    'Location Sales Volume Actual': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was true with the values we will use for ranking the suppliers, it is also common practice to normalize the weights. We normalize the weights so that they sum to one by dividing each weight value by the sum of the weights.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {key: value/sum(weights.values()) for key, value in weights.items()}\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Weighted_Sum_Method\"> </a>\n",
    "# Technique 1: Weighted Sum Method\n",
    "\n",
    "The weighted sum approach we are going to look at is arguably the best known and simplest method for evaluating a set of alternatives according to a number of criteria. \n",
    "\n",
    ">[The weighted sum method determines a] weighted point [estimate for each alternative], which consider attributes that are weighted by the buyer. The weight for each attribute is then multiplied by the performance score that is assigned. Finally, these products are totaled to determine final rating for each supplier. Typically this system is designed to utilize quantitative measurements. The advantages of the weighted point method include the ability for the organization to include numerous evaluation factors and assign them weights according to the organizationâ€™s needs. The subjective factors on the evaluation are minimized. The major limitation of this approach is that it is difficult to effectively take qualitative evaluation criteria into consideration.\n",
    ">\n",
    "> - Khaled, A.A., Paul, S. K., Ripon Kumar Chakrabortty, M., & Ayuby, S. (2011). Selection of suppliers through different multi-criteria decision making techniques. Global Journal of Management and Business Research, 11(4).\n",
    "\n",
    "Assume that we want to evaluate $m$ alternatives (suppliers) on $n$ criteria and that higher values of all criteria are better. Let $w_{j}$ denote the relative weight of criterion $j$ and $a_{ij}$ denote the score of alternative $i$ in terms of criterion $j$. Using the described notation, the weighted sum score for alternative $i$ is given by:\n",
    "\n",
    "$$\\sum_{j=1}^{n}w_{j}a_{ij},\\mbox{ for }i=1,2,3,\\ldots,m.$$\n",
    "\n",
    "The following code block shows how we can perform the weighted sum evaluation using our data and weights.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty pandas series with the same index as our data\n",
    "# and all values initialized to zero\n",
    "temp = pd.Series(index = clean_data.index, data = 0)\n",
    "\n",
    "# for each key in our weight dictionary\n",
    "for key, weight in weights.items():\n",
    "    \n",
    "    # if the key corresponds to a column in our data\n",
    "    if key in clean_data.columns:\n",
    "        \n",
    "        # increment the value of the series by the product\n",
    "        # of the weight times the column values\n",
    "        temp += clean_data[key]*weight\n",
    "        \n",
    "    # if the key does not correspond to a column\n",
    "    else:\n",
    "        \n",
    "        # pass\n",
    "        continue\n",
    "\n",
    "# convert the series to a list\n",
    "temp = temp.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block prints the first five weight values.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block specifies a function that computes the weighted sum calculations.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_sum(data, weights_dict):\n",
    "    \"\"\"\n",
    "    Computes weighted sum for specified columns\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    data : a pandas DataFrame object that contains the columns to be weighted and the\n",
    "    normalized scores for each alternative\n",
    "    \n",
    "    weights_dict: a dictionary containing the columns to be included in the weighted sum as keys,\n",
    "    and the associated values as values\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    A list specifying the weighted sum calculations\n",
    "           \n",
    "    \"\"\"   \n",
    "\n",
    "    temp = pd.Series(index = data.index, data = 0)\n",
    "\n",
    "    for key, weight in weights_dict.items():\n",
    "        if key in data.columns:\n",
    "            temp += data[key]*weight\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return temp.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how the function may be used to create a column in our `clean_data` object that specifies the weighted sum scores.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['WS'] = compute_weighted_sum(clean_data, weights)\n",
    "\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Weighted_Product_Method\"> </a>\n",
    "# Technique 2: Weighted Product Method\n",
    "\n",
    "We will now consider the weighted product method for ranking alternatives. Using the notation we defined earlier, the weighted product score for alternative $A_{i}$ is given by:\n",
    "\n",
    "$$\\prod_{j=1}^{n}(a_{ij})^{w_{j}},\\mbox{ for }i=1,2,3,\\ldots,m.$$\n",
    "\n",
    "The following code block defines a function that determines the weighted product ranking for a set of alternatives. A key thing to note is that instead of initializing all values of the series to zero, we must initialize them to one for the weighted product to work.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_product(data, weights_dict):\n",
    "    \"\"\"\n",
    "    Computes weighted product for specified columns\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    data : a pandas DataFrame object that contains the columns to be weighted and the\n",
    "    normalized scores for each alternative\n",
    "    \n",
    "    weights_dict: a dictionary containing the columns to be included in the weighted product as keys,\n",
    "    and the associated values as values\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    A list specifying the weighted product calculations\n",
    "           \n",
    "    \"\"\"   \n",
    "    \n",
    "    temp = pd.Series(index = data.index, data = 1)\n",
    "    for key, weight in weights_dict.items():\n",
    "        if key in data.columns:\n",
    "            temp *= (data[key]**weight)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return temp.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how the function may be used to create a column in our `clean_data` object that specifies the weighted sum scores.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['WP'] = compute_weighted_product(clean_data, weights)\n",
    "\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block plots the weighted sum and weighted product scores using a scatterplot. Is there anything that we can say in general regarding the two scoring methods? What do you think is the cause for the weighted product evaluations that equal zero?\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure object\n",
    "fig, ax = plt.subplots(1, 1, figsize = (6, 6))\n",
    "\n",
    "# create scatterplot\n",
    "sns.scatterplot(x = 'WS',\n",
    "                y = 'WP',\n",
    "                s = 100,\n",
    "                alpha = 0.3,\n",
    "                edgecolor = 'k',\n",
    "                data = clean_data)\n",
    "\n",
    "# update x and y (horizontal and vertical) axis limits\n",
    "ax.set_xlim(-0.02, 1.02)\n",
    "ax.set_ylim(-0.02, 1.02)\n",
    "\n",
    "# modify labels\n",
    "ax.set_xlabel('Weighted Sum Score', fontsize = 12)\n",
    "ax.set_ylabel('Weighted Product Score', fontsize = 12)\n",
    "\n",
    "# plot diagonal line\n",
    "ax.plot(ax.get_xlim(),\n",
    "        ax.get_ylim(), \n",
    "        ls = \"--\", \n",
    "        color = 'k',\n",
    "        )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mcdm_functions\"> </a>\n",
    "# The mcdm_functions module\n",
    "\n",
    "Up to this point, we have built functions in the current notebook to apply our ranking methods. In practice, we probably would like to define the functions in separate files that we can import like `pandas` or `numpy`. The following code block imports such a `library`, named `OM527_functions`, which should be a folder in the current working directory (the directory where this notebook resides). **Note: Although *Python packaging* is beyond the scope of this course, it is relatively straightforward and many good tutorials and references are available online.**\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import OM527_functions as omf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `OM527_functions` contains two main modules:\n",
    "1. the `mcdm` module that contains functions for multi-criteria decision-making tasks, and\n",
    "2. the `distfit` module that contains functions for distribution fitting.\n",
    "\n",
    "In this notebook, we will be utilizing the `mcdm` module. The following code block reinitializes the `clean_data` object that we created earlier.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_columns = ['Company Name']\n",
    "\n",
    "agg_dict = {\n",
    "    'Location Sales Volume Actual': ['sum'],\n",
    "    'Open Saturday': ['mean'],\n",
    "    'Open Sunday': ['mean'],\n",
    "    'Credit Score Num': ['mean'],\n",
    "    'Zip Code Score': ['mean'],\n",
    "}\n",
    "\n",
    "clean_data = tr_data.groupby(groupby_columns).agg(agg_dict)\n",
    "clean_data.columns = agg_dict.keys()\n",
    "clean_data = clean_data.reset_index()\n",
    "clean_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will use the same weights. The dictionary of weights is recreated below.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'Open Saturday': 3,\n",
    "           'Open Sunday': 2, \n",
    "           'Credit Score Num': 9, \n",
    "           'Zip Code Score': 7,\n",
    "           'Location Sales Volume Actual': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the `mcdm` module of the imported library, which we imported with the alias `omf`, using the syntax `omf.mcdm`. The `mcdm` module includes a function named `normalize_weights_dictionary`. This function accepts a dictionary of weights as an input, and returns a copy of the dictionary with the weight values normalized. This is demonstrated below.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = omf.mcdm.normalize_weights_dictionary(weights)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in our *refreshed* version of the data, the `Location Sales Volume Actual` column is not normalized. The `mcdm` module contains a function named `normalize_array` that uses the `MinMaxScaler` from scikit-learn to normalize a single array of numbers proved as a list, a pandas Series, or a numpy array.  This is demonstrated below.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['Location Sales Volume Actual'] = omf.mcdm.normalize_array(clean_data['Location Sales Volume Actual'])\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mcdm` module also contains functions that allow us to compute the weighted sum (`compute_weighted_sum`) and weighted product (`compute_weighted_product`) scores for a provided DataFrame. The values in the passed DataFrame should be normalized before running any of these functions. The functions also accept a dictionary of weights, where the `keys` match columns in the DataFrame. The weight `values` in the dictionary should be normalized. The following code block shows how we can apply these two functions.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['WS'] = omf.mcdm.compute_weighted_sum(clean_data, weights)\n",
    "clean_data['WP'] = omf.mcdm.compute_weighted_product(clean_data, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five rows of the data, after applying our functions, are shown below.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"AHP\"> </a>\n",
    "# Weight Determination via Analytic Hierarchy Process (AHP)\n",
    "\n",
    "We will now turn our attention to setting weights for a supplier ranking task. This can be a particularly challenging process because the fact that we are including an attribute means it is important to some degree. In this section, we will investigate a technique known as Analytics Heirarchy Process for setting weights. From https://en.wikipedia.org/wiki/Analytic_hierarchy_process (accessed 2/4/2019):\n",
    "\n",
    "> The analytic hierarchy process (AHP) is a structured technique for organizing and analyzing complex decisions, based on mathematics and psychology. It was developed by Thomas L. Saaty in the 1970s and has been extensively studied and refined since then.\n",
    ">\n",
    "> Rather than prescribing a \"correct\" decision, the AHP helps decision makers find one that best suits their goal and their understanding of the problem. It provides a comprehensive and rational framework for structuring a decision problem, for representing and quantifying its elements, for relating those elements to overall goals, and for evaluating alternative solutions.\n",
    ">\n",
    "> Decision makers systematically evaluate its various [attributes] by comparing them to each other two at a time, with respect to their impact on [a predetermined objective]. In making the comparisons, the decision makers can use concrete data about the elements, but they typically use their judgments about the elements' relative meaning and importance. It is the essence of the AHP that human judgments, and not just the underlying information, can be used in performing the evaluations.\n",
    ">\n",
    "> The AHP converts these evaluations to numerical values that can be processed and compared over the entire range of the problem. A numerical weight or priority is derived for each [attribute], allowing diverse and often incommensurable elements to be compared to one another in a rational and consistent way. This capability distinguishes the AHP from other decision making techniques.\n",
    ">\n",
    "> In the final step of the process, numerical priorities are calculated for each of the decision alternatives. These numbers represent the alternatives' relative ability to achieve the decision goal, so they allow a straightforward consideration of the various courses of action.\n",
    "\n",
    "The pairwise evaluations performed in an AHP analysis follow the rubric provided in the table that follows.\n",
    "\n",
    "<table border=\"1\" class=\"colwidths-given docutils\">\n",
    "<colgroup>\n",
    "<col width=\"20%\" />\n",
    "<col width=\"20%\" />\n",
    "<col width=\"60%\" />\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\" style=\"text-align:center\"><th class=\"head\">Intensity of Importance</th>\n",
    "<th class=\"head\" style=\"text-align:left\">Definition</th>\n",
    "<th class=\"head\" style=\"text-align:left\">Explanation</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">$\\frac{1}{9} \\approx 0.111$</td>\n",
    "    <td style=\"text-align:left\">Extreme Unimportance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is extremely less preferable than criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">$\\frac{1}{7}\\approx 0.143$</td>\n",
    "    <td style=\"text-align:left\">Very Strong Unimportance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is very strongly less preferable than criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">$\\frac{1}{5} = 0.2$</td>\n",
    "    <td style=\"text-align:left\">Strong Unimportance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is strongly less preferable than criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">$\\frac{1}{3}\\approx 0.333$</td>\n",
    "    <td style=\"text-align:left\">Moderate Unimportance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is moderately less preferable than criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">1</td>\n",
    "    <td style=\"text-align:left\">Equal Importance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is equally important as criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">3</td>\n",
    "    <td style=\"text-align:left\">Moderate Importance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is moderately preferable to criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">5</td>\n",
    "    <td style=\"text-align:left\">Strong Importance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is strongly preferable to criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">7</td>\n",
    "    <td style=\"text-align:left\">Very Strong Importance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is very strongly preferable to criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">9</td>\n",
    "    <td style=\"text-align:left\">Extreme Importance</td>\n",
    "    <td style=\"text-align:left\">Criterion A is extremely preferable to criterion B with respect to the objective</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "A challenging aspect of performing an AHP analysis in Python is collecting the evaluation data and ensuring everything is in the correct format. Thus, we will utilize an automated approach that will be demonstrated shortly. First, let's use a small dataset on cereal preferences to demonstrate how AHP works.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = [\n",
    "    [1, 5, 9],\n",
    "    [0.2, 1, 4],\n",
    "    [0.111, 0.25, 1]\n",
    "]\n",
    "\n",
    "index_vals = ['Lucky Charms', 'Trix', 'Frosted Flakes']\n",
    "\n",
    "cereal_data = pd.DataFrame(comparison_data, index = index_vals, columns = index_vals)\n",
    "cereal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `values` attribute of a pandas DataFrame to get the values as a numpy array.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cereal_scores = cereal_data.values\n",
    "cereal_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Calculate column sums for the evaluation matrix.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums = np.sum(cereal_scores,axis=0)\n",
    "column_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Divide all scores in the evaluation matrix by the column sums.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_divided_by_sums = cereal_scores/column_sums\n",
    "scores_divided_by_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Create a prority vector by determining the average score in each row of the updated evaluation matrix.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_vector = np.average(scores_divided_by_sums, axis=1)\n",
    "priority_vector   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Approximate the maximum eigenvalue of the evaluation matrix by computing the inner product of the priority vector and the column sums.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eigenvalue = np.inner(priority_vector, column_sums)\n",
    "max_eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Compute the consistency index (CI) for the evaluation matrix using the formula:\n",
    "\n",
    "$$ CI = (\\lambda - n)/(n-1),$$\n",
    "\n",
    "where $\\lambda$ represents the maximum eigenvalue and $n$ represents the number of attributes.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_CI = (max_eigenvalue - len(priority_vector))/(len(priority_vector)-1)\n",
    "criteria_CI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "\n",
    "Compute the consistency ratio (CR) using the formula:\n",
    "\n",
    "$$ CI/RI_{n},$$\n",
    "\n",
    "where $RI_{n}$ is given in the following table.\n",
    "\n",
    "<table border=\"1\" class=\"colwidths-given docutils\">\n",
    "<colgroup>\n",
    "<col width=\"20%\" />\n",
    "<col width=\"20%\" />\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\" style=\"text-align:center\">\n",
    "    <th class=\"head\" style=\"text-align:center\">$n$</th>\n",
    "    <th class=\"head\" style=\"text-align:center\">$RI_{n}$\n",
    "    </th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">0</td>\n",
    "    <td style=\"text-align:center\">0</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">1</td>\n",
    "    <td style=\"text-align:center\">0</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">2</td>\n",
    "    <td style=\"text-align:center\">0</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">3</td>\n",
    "    <td style=\"text-align:center\">0.58</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">4</td>\n",
    "    <td style=\"text-align:center\">0.9</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">5</td>\n",
    "    <td style=\"text-align:center\">1.12</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">6</td>\n",
    "    <td style=\"text-align:center\">1.24</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">7</td>\n",
    "    <td style=\"text-align:center\">1.32</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">8</td>\n",
    "    <td style=\"text-align:center\">1.41</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">9</td>\n",
    "    <td style=\"text-align:center\">1.45</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\">\n",
    "    <td style=\"text-align:center\">10</td>\n",
    "    <td style=\"text-align:center\">1.49</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "If $CR < 0.10$, the evaluations are considered to be consistent. The following code block applies this consistency check to our example data. In this case, our data is just over the previously described consitency threshold, but it is probably fine to use.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RI = [0, 0, 0, 0.58, 0.9, 1.12, 1.24, 1.32, 1.41, 1.45, 1.49] \n",
    "criteria_CR = np.round(criteria_CI/RI[len(priority_vector)], 5)\n",
    "criteria_CR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block reruns the example with a set of data that includes inconsistent preferences.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = [\n",
    "    [1, 5, 9],\n",
    "    [0.2, 1, 0.25],\n",
    "    [0.111, 4, 1]\n",
    "]\n",
    "\n",
    "index_vals = ['Lucky Charms', 'Trix', 'Frosted Flakes']\n",
    "\n",
    "cereal_data = pd.DataFrame(comparison_data, index = index_vals, columns = index_vals)\n",
    "\n",
    "cereal_scores = cereal_data.values\n",
    "\n",
    "column_sums = np.sum(cereal_scores, axis=0)\n",
    "\n",
    "scores_divided_by_sums = cereal_scores/column_sums\n",
    "\n",
    "priority_vector = np.average(scores_divided_by_sums, axis=1)\n",
    "\n",
    "max_eigenvalue = np.inner(priority_vector, column_sums)\n",
    "\n",
    "criteria_CI = (max_eigenvalue - len(priority_vector))/(len(priority_vector)-1)\n",
    "\n",
    "criteria_CR = np.round(criteria_CI/RI[len(priority_vector)], 5)\n",
    "\n",
    "print(f'The consistency ratio is {criteria_CR}')\n",
    "\n",
    "cereal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mcdm module includes a function named `create_ahp_rank_matrix_from_gui` that we will use to automate the application of AHP. The function expects a `list` of criteria that should be compared. Suppose that the following list provides our order of relative importance for the selected criteria.\n",
    "\n",
    "1. `Credit Score Num`\n",
    "2. `Location Sales Volume Actual`\n",
    "3. `Zip Code Score`\n",
    "4. `Open Saturday`\n",
    "5. `Open Sunday`\n",
    "\n",
    "Moreover, assume that we have the following preferences using the language of AHP.\n",
    "\n",
    "- `Credit Score Num` is **moderately more preferable** than `Location Sales Volume Actual`\n",
    "- `Credit Score Num` is **strongly more preferable** than `Zip Code Score`\n",
    "- `Credit Score Num` is **very strongly more preferable** than `Open Saturday`\n",
    "- `Credit Score Num` is **extremely more preferable** than `Open Sunday`\n",
    "- `Location Sales Volume Actual` is **moderately more preferable** than `Zip Code Score`\n",
    "- `Location Sales Volume Actual` is **strongly more preferable** than `Open Saturday`\n",
    "- `Location Sales Volume Actual` is **very strongly more preferable** than `Open Sunday`\n",
    "- `Zip Code Score` is **moderately more preferable** than `Open Saturday`\n",
    "- `Zip Code Score` is **strongly more preferable** than `Open Sunday`\n",
    "- `Open Saturday` is **moderately more preferable** than `Open Sunday`\n",
    "\n",
    "You may correctly suspect that we can just past the `keys` of our `weights` dictionary as a `list` to the `create_ahp_rank_matrix_from_gui` function. The following code block shows how to generate such a list.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(weights.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although passing such a list is perfectly fine, it is best to provide the criteria in the order of decreasing expected importance. Such a representation for our criteria follows.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_list = ['Credit Score Num',\n",
    " 'Location Sales Volume Actual',\n",
    " 'Zip Code Score',\n",
    " 'Open Saturday',\n",
    " 'Open Sunday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block applies the function.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will demo this in class\n",
    "#weights = omf.mcdm.create_ahp_rank_matrix_from_gui(criteria_list)\n",
    "\n",
    "weights = {'Credit Score Num': 0.5028194957704966,\n",
    "           'Location Sales Volume Actual': 0.2602315877866833,\n",
    "           'Zip Code Score': 0.1343504405731109,\n",
    "           'Open Saturday': 0.06777766684747813,\n",
    "           'Open Sunday': 0.03482080902223112}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply our weights to our data, overwriting the previous scores.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['WS'] = omf.mcdm.compute_weighted_sum(clean_data, weights)\n",
    "clean_data['WP'] = omf.mcdm.compute_weighted_product(clean_data, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block generates scatterplots for the scores found by the two methods.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure object\n",
    "fig, ax = plt.subplots(1, 1, figsize = (6, 6))\n",
    "\n",
    "# create scatterplot\n",
    "sns.scatterplot(x = 'WS',\n",
    "                y = 'WP',\n",
    "                s = 100,\n",
    "                alpha = 0.3,\n",
    "                edgecolor = 'k',\n",
    "                data = clean_data)\n",
    "\n",
    "# update x and y (horizontal and vertical) axis limits\n",
    "ax.set_xlim(-0.02, 1.02)\n",
    "ax.set_ylim(-0.02, 1.02)\n",
    "\n",
    "# modify labels\n",
    "ax.set_xlabel('Weighted Sum Score', fontsize = 12)\n",
    "ax.set_ylabel('Weighted Product Score', fontsize = 12)\n",
    "\n",
    "# plot diagonal line\n",
    "ax.plot(ax.get_xlim(),\n",
    "        ax.get_ylim(), \n",
    "        ls = \"--\", \n",
    "        color = 'k',\n",
    "        )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Ensemble\"> </a>\n",
    "# Ensemble Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final topic that we will consider is ensemble methods for supplier selection. The following code block refreshes our data and weights.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_columns = ['Company Name']\n",
    "\n",
    "agg_dict = {\n",
    "    'Location Sales Volume Actual': ['sum'],\n",
    "    'Open Saturday': ['mean'],\n",
    "    'Open Sunday': ['mean'],\n",
    "    'Credit Score Num': ['mean'],\n",
    "    'Zip Code Score': ['mean'],\n",
    "}\n",
    "\n",
    "clean_data = tr_data.groupby(groupby_columns).agg(agg_dict)\n",
    "clean_data.columns = agg_dict.keys()\n",
    "clean_data['Location Sales Volume Actual'] = omf.mcdm.normalize_array(clean_data['Location Sales Volume Actual'])\n",
    "clean_data = clean_data.reset_index()\n",
    "\n",
    "weights = {'Credit Score Num': 0.5028194957704966,\n",
    "           'Location Sales Volume Actual': 0.2602315877866833,\n",
    "           'Zip Code Score': 0.1343504405731109,\n",
    "           'Open Saturday': 0.06777766684747813,\n",
    "           'Open Sunday': 0.03482080902223112}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in our earlier discussion of the weighted sum, weighted product, and TOPSIS ranking methods, there are differences in the rankings produced by each method. Also, whenever we perform a ranking, the weights that we use are based on our preferences at the time that the weights are deterimined. Moreover, if we use a technique such as AHP, the waits my include some inherent bias that is associated with the decision makers involved in deriving the pairiwse attribute comparisons. \n",
    "\n",
    "Based on these complications, we will look at the use of ensemble methods to 1) ensure that our supplier rankings are robust to variations in weights and 2) incorporate insights from all three ranking methods.\n",
    "\n",
    "The following code block defines a function that performs a robust ranking evaluation for a specified scoring method. The function allows a user to define a perturbation range that is used to randomly vary the weights for the various criteria in the range +/- the defined perturbation. This is done for a specified number of times and the number of times that a particular supplier appears as a top supplier is counted and plotted. The function returns a `DataFrame` object with the robust ranking results.\n",
    "\n",
    "**Additional details on the operation of this function will be provided in class.**\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_ranking(data,\n",
    "                   weights_dict,\n",
    "                   index_column,\n",
    "                   ranking_methods_dict,\n",
    "                   perturbation_range = 0.1,\n",
    "                   top_values = 5,\n",
    "                   perturbations = 100):\n",
    "    \n",
    "    '''\n",
    "    Performs a robust ranking procedure that identfies the proportion of times where\n",
    "    the available alternatives appear as top choice under weight perturbations in the\n",
    "    range defined by the perturbation_range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : a pandas DataFrame object that contains the specified columns\n",
    "        \n",
    "    weights_dict: a dictionary containing the columns to be included in the weighted product as keys,\n",
    "    and the associated values as values\n",
    "    \n",
    "    index_column: a string specifying the column of alternatives\n",
    "    \n",
    "    ranking_methods_dict: a dictionary with each key specifying a name for each ranking method\n",
    "    to be used and the corresponding values specifying the function. Each dunction should take\n",
    "    the data object and the weights_dict object as arguments and return a list with the\n",
    "    scores for each alternative.\n",
    "           \n",
    "    perturbations: an integer specifying the number of times to perturb the weights\n",
    "    \n",
    "    top_values: specifies the number of alternatives to keep from each ranking (highest ranked scores kept)\n",
    "    \n",
    "    perturbation_range: The perturbation range to consider (expressed as a proportion)    \\\n",
    "    \n",
    "    random_seed: float specifying seed for random number generator\n",
    "    \n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    a DataFrame indicating the proportion of times each alternative appears in top ranking\n",
    "    '''\n",
    "\n",
    "    criteria = list(weights_dict.keys())\n",
    "    \n",
    "    a = np.zeros(shape = (len(data.index), \n",
    "                          len(ranking_methods_dict.keys())+1)\n",
    "                )\n",
    "\n",
    "    counts_df = pd.DataFrame(a, columns=[index_column] + list(ranking_methods_dict.keys()))\n",
    "\n",
    "    counts_df[index_column] = data[index_column]\n",
    "\n",
    "    starting_weights = np.array(list(weights_dict.values()))\n",
    "\n",
    "    for perturbation in range(perturbations):\n",
    "        np.random.seed(perturbation)\n",
    "\n",
    "        perturbation_vector = 1.0 + np.random.uniform(low = -1.0*perturbation_range,\n",
    "                                                      high = perturbation_range,\n",
    "                                                      size= len(starting_weights))\n",
    "\n",
    "        perturbed_weights = perturbation_vector * starting_weights\n",
    "        perturbed_weights = list(perturbed_weights/perturbed_weights.sum())\n",
    "        perturbed_weights_dict = dict(zip(criteria, perturbed_weights))\n",
    "\n",
    "        for current_ranking_method in ranking_methods_dict.keys():\n",
    "\n",
    "            temp = pd.DataFrame(ranking_methods_dict[current_ranking_method](data, perturbed_weights_dict),\n",
    "                                index = data[index_column],\n",
    "                                columns = [current_ranking_method])\n",
    "\n",
    "            top_index_vals = temp.nlargest(top_values, \n",
    "                                           current_ranking_method).index.tolist()\n",
    "\n",
    "            counts_df.loc[counts_df[index_column].isin(top_index_vals), current_ranking_method] += 1\n",
    "\n",
    "    counts_df[list(ranking_methods_dict.keys())] = counts_df[list(ranking_methods_dict.keys())]/perturbations\n",
    "    counts_df = counts_df.sort_values(list(ranking_methods_dict.keys()), ascending = False)\n",
    "    mask = counts_df[list(ranking_methods_dict.keys())].sum(axis = 1) > 0\n",
    "    counts_df = counts_df[mask]\n",
    "    \n",
    "    return counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how we can use the function to identify the top 5 suppliers, by each ranking method, when we allow the attribute weights to vary within 30% of the mean weight value.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_methods = {'WS': omf.mcdm.compute_weighted_sum,\n",
    "                   'WP': omf.mcdm.compute_weighted_product}\n",
    "\n",
    "robust_rankings = robust_ranking(clean_data,\n",
    "                                 weights,\n",
    "                                 'Company Name',\n",
    "                                 ranking_methods,\n",
    "                                 perturbation_range = 0.30,\n",
    "                                 top_values = 5,\n",
    "                                 perturbations = 100)\n",
    "\n",
    "robust_rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the results follows.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12,5))\n",
    "\n",
    "melted = robust_rankings.melt(id_vars = ['Company Name'])\n",
    "melted = melted.rename(columns = {'value':'Proportion',\n",
    "                                  'variable': 'Method'})\n",
    "\n",
    "sns.barplot(x = 'Company Name',\n",
    "            y = 'Proportion',\n",
    "            edgecolor = 'k',\n",
    "            hue = 'Method',\n",
    "            data = melted)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 45, ha=\"right\", fontsize = 12)\n",
    "ax.legend(bbox_to_anchor=(1.2, 1.05), fontsize = 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is also available in the `mcdm_functions` module. The following code block demonstrates its use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omf.mcdm.robust_ranking(clean_data,\n",
    "                        weights,\n",
    "                        'Company Name',\n",
    "                        ranking_methods,\n",
    "                        perturbation_range = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
