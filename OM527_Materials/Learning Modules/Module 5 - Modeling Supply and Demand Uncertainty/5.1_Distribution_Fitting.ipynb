{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Distribution Fitting\n",
    "Prepared by: Nickolas Freeman, PhD\n",
    "\n",
    "In this notebook, we will look at methods for fitting distributions to data using Python. In particular, we will look at both parametric and non-parametric distribution fitting methods. \n",
    "\n",
    "From https://en.wikipedia.org/wiki/Parametric_statistics (accessed 12/31/2018):\n",
    "> Parametric statistics is a branch of statistics which assumes that sample data comes from a population that follows a probability distribution based on a fixed set of parameters. Most well-known elementary statistical methods are parametric. Conversely a non-parametric model differs precisely in that the parameter set (or feature set in machine learning) is not fixed and can increase, or even decrease if new relevant information is collected.\n",
    ">\n",
    "> Since a parametric model relies on a fixed parameter set, it assumes more about a given population than non-parametric methods do. When the assumptions are correct, parametric methods will produce more accurate and precise estimates than non-parametric methods, i.e. have more statistical power. However, as more is assumed by parametric methods, when the assumptions are not correct they have a greater chance of failing, and for this reason are not robust statistical methods. On the other hand, parametric formulae are often simpler to write down and faster to compute. For this reason their simplicity can make up for their lack of robustness, especially if care is taken to examine diagnostic statistics.\n",
    "\n",
    "The parametric methods that we look at will attempt to estimate the parameters of a known empirical distribution from a data set. Essentially, we will define a set of known probability distributions, attempt to fit each one to a given data set, and then compare the errors associated with the various distribution fits to identify a single distribution that provides the closest approximation.\n",
    "\n",
    "The non-parametric method that we will consider is known as kernel density estimation. Kernel density estimation works by centering a fixed *kernel function* at each data point that composes a data set. An example of a *kernel function* is a Normal distribution with mean equal to the data point value and a standard deviation of 1.0. Summing the kernal functions for all values in the sample space, and then normalizing the resulting sums results in a estimate of the probability distribution function for the data. The smoothness of the kernel density estimate (KDE) is controlled by the kernel function. For example, using a Normal distribution with a standard deviation of 2.0 will result in a smoother KDE than a Normal distribution with a standard deviation of 1.0.\n",
    "\n",
    "Before discussing either approach to distribution fitting, we will first discuss the detection and removal of outliers from a data set. \n",
    "\n",
    "# Table of Contents\n",
    "<a id=\"Table_of_Contents\"> </a>\n",
    "\n",
    "1. [Outlier Detection and Removal](#Outlier_Detection_and_Removal)<br>\n",
    "2. [Parametric Distribution Fitting](#Parametric_Distribution_Fitting)<br>\n",
    "3. [Non-Parametric Distribution Fitting](#nonParametric_Distribution_Fitting)<br>\n",
    "4. [Putting it all together](#Putting_it_all_together)<br>\n",
    "\n",
    "The following code block imports packages that we will use to demonstrate parametric distribution fitting. In partiucular, we will largely rely on the capabilities provided by the `scipy.stats` module. From https://en.wikipedia.org/wiki/SciPy (accessed 12/31/2019):\n",
    "\n",
    "> SciPy (pronounced \"Sigh Pie\") is a free and open-source Python library used for scientific computing and technical computing.\n",
    ">\n",
    ">SciPy contains modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering.\n",
    ">\n",
    ">SciPy builds on the NumPy array object and is part of the NumPy stack which includes tools like Matplotlib, pandas and SymPy, and an expanding set of scientific computing libraries. This NumPy stack has similar users to other applications such as MATLAB, GNU Octave, and Scilab. The NumPy stack is also sometimes referred to as the SciPy stack.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block uses `numpy` to randomly generate a 500-point data set that follows a lognormal distribution. This data is plotted as a histogram. \n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data  = np.random.lognormal(mean = 0.5, sigma = 0.5, size = 1000)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12, 6))\n",
    "\n",
    "ax.hist(data, edgecolor = 'k', label = 'Actual Data')\n",
    "ax.set_ylabel('Frequency', fontsize = 20)\n",
    "ax.set_xlabel('Value', fontsize = 20)\n",
    "ax.set_title('Histogram of Data',  fontsize = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection and Removal\n",
    "<a id=\"Outlier_Detection_and_Removal\"> </a>\n",
    "\n",
    "In this section, we will discuss the detection and removal of outliers from a data set. The most common approach for this purpose is the use of boxplots (see https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51 (accessed 2/27/2020) for additional discussion.\n",
    "\n",
    "> Boxplots are a standardized way of displaying the distribution of data based on a five number summary (\"minimum\", first quartile (Q1), median, third quartile (Q3), and \"maximum\"), where\n",
    "> - the median (Q2/50th Percentile) denotes the middle value of the dataset,\n",
    "> - the first quartile (Q1/25th Percentile) denotes the middle number between the smallest number (not the \"minimum\") and the median of the dataset,\n",
    "> - the third quartile (Q3/75th Percentile) denotes the middle value between the median and the highest value (not the \"maximum\") of the dataset,\n",
    "> - the interquartile range (IQR) denotes the difference between the third and first quartiles,\n",
    "> - the \"maximum\" is estimated as Q3 + 1.5(IQR), and\n",
    "> - the \"minimum\" is estimated as Q1 -1.5(IQR).\n",
    "\n",
    "Essentially, the boxlot calculations use the interquartile range (IQR) to approximate the variability associated with the data, and estimate the minimum and maximum values using multiples of the IQR. These estimations of the minimum and maximum values are not the smallest and largest values in the data. Instead, they are approximations for *reasonable* boundaries of the distribution. Any values that are less than or greater than these boundaries are flagged as outliers.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Analysts should exercise extreme care when using boxplots to identify outliers when the underlying data is skewed. In particular, traditional boxplots implicitly assume that the underlying data is symmetric. If applied to skewed data, the traditional method for detecting outliers can lead to incorrectly identifying points as outliers.  \n",
    "</div>\n",
    "    \n",
    "Given the sensitivity of the traditional approach for outlier detection to data skewness, we will employ the publised in Walker et al. (2018). In this research, the authors propose a new method for identifying outliers that is based on a statistic called Bowley’s Coefficient. Using this statistic, the authors devise a new approach for detecting outliers that is robust and better able to accommodate skewed data. The citation for the paper, which was published in *The American Statistician* is:\n",
    "\n",
    "Walker, M. L., Dovoedo, Y. H., Chakraborti, S., & Hilton, C. W. (2018). An improved boxplot for univariate data. The American Statistician, 72(4), 348-353.\n",
    "\n",
    "The following code block defines a function for handling outliers that defaults to the method described in Walker et al. (2018). However, it also allows users to force the assumption of symmetric data.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(data, outlier_threshold = 3.0, assume_symmetric = False, return_bool_array = False):\n",
    "    '''\n",
    "    This function removes outliers from a dataset that is structured\n",
    "    as a 1-dimensional numpy array, pandas seriers, or list. In contrast\n",
    "    to using traditional boxplots, the defualt for this function is based on \n",
    "    the outlier detection method described in Walker et al. (2018). A citation for\n",
    "    the paper follows. In contrast to more traditional methods, this approach\n",
    "    does not assume that the data is symmetric. If a user wants to force the\n",
    "    assumption of symmetry, they may do so using the optional assume_symmetric\n",
    "    argument. By default, the function returns a list that conatins the data\n",
    "    with outliers removed. If the user wants to inspect the data points flagged\n",
    "    as outliers, the return_bool_array argument may be specified to return a list\n",
    "    of boolean values with True indicating that a point is NOT an outlier and\n",
    "    False indicating that a point IS an outlier.\n",
    "    \n",
    "    The citation for the boxplot method employed for non-symmetric data is:\n",
    "    Walker, M. L., Dovoedo, Y. H., Chakraborti, S., & Hilton, C. W. (2018). \n",
    "    An improved boxplot for univariate data. \n",
    "    The American Statistician, 72(4), 348-353.\n",
    "    \n",
    "    Arguments:\n",
    "    data: a 1-dimensional numpy array, pandas series, or python list that \n",
    "    includes the data\n",
    "    \n",
    "    assume_symmetric: True or False to indicate whether or not the assumption\n",
    "    of symmetrically distributed data should be enforced (default = False)\n",
    "    \n",
    "    return_bool_array: True or False to indicate whether or not to return a\n",
    "    list of values with the outliers removed (False) or a list of boolean\n",
    "    values where True indicates that a point is NOT an outlier and\n",
    "    False indicates that a point IS an outlier\n",
    "    \n",
    "    Returns:\n",
    "    By default, the function returns a list that conatins the data\n",
    "    with outliers removed. If the user wants to inspect the data points flagged\n",
    "    as outliers, the return_bool_array argument may be specified to return a list\n",
    "    of boolean values with True indicating that a point is NOT an outlier and\n",
    "    False indicating that a point IS an outlier.\n",
    "             \n",
    "    '''\n",
    "    import numpy as np\n",
    "    \n",
    "    # Convert data to a numpy array\n",
    "    data = np.array(data)     \n",
    "        \n",
    "    # calculate the 25th, 50th, and 75th percentiles    \n",
    "    q1, q2, q3 = np.nanpercentile(data, [25, 50, 75])\n",
    "    \n",
    "    # calculate the interquartile range\n",
    "    IQR = q3 - q1\n",
    "    \n",
    "    # if user wants to force the assumption that\n",
    "    # data is symmetric\n",
    "    if assume_symmetric:\n",
    "        \n",
    "        # set ratios for lower and upper fences to 1\n",
    "        RL = 1\n",
    "        RU = 1\n",
    "        \n",
    "    # if user wants to use non-symmetric method\n",
    "    else:    \n",
    "        # Calculate Bowley’s Coefficient\n",
    "        BC = (q3 + q1 - 2*q2)/(q3-q1)\n",
    "\n",
    "        # Calculate ratio for lower fence\n",
    "        RL = (1 - BC)/(1 + BC)\n",
    "        \n",
    "        # Calculate ratio for upper fence\n",
    "        RU = (1 + BC)/(1 - BC)\n",
    "\n",
    "    # compute upper and lower fences\n",
    "    FL = q1 - outlier_threshold*IQR*RL\n",
    "    FU = q3 + outlier_threshold*IQR*RU\n",
    "        \n",
    "    # Calculate values between lower and upper fences\n",
    "    mask = np.logical_not((data >= FU) | (data <= FL))\n",
    "    \n",
    "    # if return_bool_array is True\n",
    "    if return_bool_array:\n",
    "        # return mask as a list\n",
    "        return mask.tolist()\n",
    "    \n",
    "    else: \n",
    "        # return list of values with outliers removed\n",
    "        return data[np.logical_not((data > FU) | (data < FL))].tolist()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block constructs a histogram that shows the original data along with the copies of the data that have outliers removed.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "@interact(view = ['Data', 'Walker et al. (2018) Method', 'Traditional'])\n",
    "\n",
    "def interactive_plot(view = 'Data'):\n",
    "    \n",
    "    view_dict = {'Data': data, \n",
    "                 'Walker et al. (2018) Method': handle_outliers(data), \n",
    "                 'Traditional': handle_outliers(data, assume_symmetric = True)}\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (12, 6))\n",
    "    \n",
    "    ax.hist(view_dict[view], \n",
    "            bins = np.arange(0, 7, 0.25),\n",
    "            density = True,\n",
    "            edgecolor = 'k', \n",
    "            color = 'g')\n",
    "    ax.set_xlabel('Value', fontsize = 15)\n",
    "    ax.set_ylabel('Frequency', fontsize = 15)\n",
    "    ax.set_title(f'Histogram for {view}', fontsize = 15)\n",
    "    ax.set_xlim([0, data.max()+1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Distribution Fitting\n",
    "<a id=\"Parametric_Distribution_Fitting\"> </a>\n",
    "\n",
    "This section looks at how we can use the `scipy` library to for parametric distribution fitting. As stated earlier, parametric statistical techniques assume that a sample of data belongs to a larger population that follows a probability distribution that is based on a fixed set of parameters, e.g., a normal distribution with mean $\\mu$ and standard deviation $\\sigma$. Our approach to fitting distributions will be as follows:\n",
    "\n",
    "1. Construct a histogram of the data,\n",
    "2. Use the bin edges of the histogram to compute the midpoints of the bins,\n",
    "3. Fit known probability distributions to the previously computed midpoints,\n",
    "4. Calculate the sum-of-squares error between the fit from step 3 and the actal data,\n",
    "5. Select a distibrution for use based on the computed errors.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Midpoints\n",
    "\n",
    "Before continuing, let's look at how we can construct a histogram for a given data set and identify the midpoints of the bins. For the histogram construction, we will use `numpy`'s `histogram` method. For a provided set of data, this function returns a tuple that specifies the height of each bin and the bin edges. **Note that if we have $n$ bins, we will have $n+1$ bin edges**. Setting the `density` argument to `True` tells the method to normalize the values returned so that it resembles a probability density function.\n",
    "\n",
    "The following code block executes the method on our sample data.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = np.histogram(data, bins = 'auto', density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for each bin are given by printing the `y` object.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `len()` function, we can see how many values are included in the `y` object.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for each bin edge are given by printing the `x` object.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted earlier, the number of bin edges returned exceeds the number of values by one.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the midpoints for the bins, we need to essentially compute the midpoint of consecutive bin edges. We will use `numpy`'s `roll` method to perform this computation. The roll function essentially allows us to shift the values in a `numpy` array by a specified amount. For example, the following code block shifts all values in the array one position earlier, with the first value *wrapping* around to the end.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.roll(x, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we add the values of the original array to the shifted array and divide by two, we will obtain the bin midpoints in the $n-1$ positions of the summed array, where $n$ is the number of bin edges. This is shown in the following code block.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x + np.roll(x, -1)) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted, it is easy to see that the first value in the computed array is the midpoint between the first two bin edges returned by the `histogram` method. To remove the last value, we simple use `numpy` array indexing to exclude the value as shown in the following code block.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x + np.roll(x, -1))[:-1] / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the length of the computed array now matches the length of the values array returned by the `histogram` method.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((x + np.roll(x, -1))[:-1] / 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function that uses the `scipy` library, along with the previously desribed midpoint calculation approach to determine how well known probability distributions fit a provided data set. Comments in the function provide insight into what is happening. This function was largely motivated by the post found at https://stackoverflow.com/questions/6620471/fitting-empirical-distribution-to-theoretical-ones-with-scipy-python (accessed 1/1/2019).\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_distributions(data, fit_all = False):\n",
    "    '''\n",
    "    This function fits all of the parametric distributions available\n",
    "    in the scipy.stats module to a provided dataset, computes the \n",
    "    sum-of-squares error (SSE) for each distribution, and returns a\n",
    "    dictionary that specifes the SSE, distribution parameters, and\n",
    "    a frozen distribution generator for each distribution. The distribution \n",
    "    generator may be used with '.rvs()' method availble in scipy.stats to \n",
    "    generate a random sample.\n",
    "    \n",
    "    Arguments:\n",
    "    data: a 1-dimensional list or Numpy array that includes the data\n",
    "    \n",
    "    fit_all: True or False to specify whether of not the function will\n",
    "    attempt to fit all available distributions. If False, only a subset of\n",
    "    common distributions are fit.\n",
    "    \n",
    "    Returns:\n",
    "    data: a dictionary that specifes the SSE, distribution parameters, and\n",
    "    a generator object for each distribution. The keys of the dictionary are\n",
    "    the index values for each distribution when sorted by SSE. Thus, the \n",
    "    distribution associated with key 0 is the best fitting distribution.\n",
    "         \n",
    "        \n",
    "    '''\n",
    "    import warnings\n",
    "    import numpy as np\n",
    "    import scipy.stats as st\n",
    "        \n",
    "    # The following lines convert the data to a histogram and\n",
    "    # compute the midpoints of the bins\n",
    "    y, x = np.histogram(data, bins='auto', density=True)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "    \n",
    "    # Initialize empty list for storing fit information\n",
    "    dist_list = []\n",
    "    dist_gen_list = []\n",
    "    sse_list = []\n",
    "    args_list = []\n",
    "\n",
    "    # Distributions to check\n",
    "    all_distributions = []\n",
    "    if fit_all:   \n",
    "        for this in dir(st):\n",
    "            if (\"fit\" in eval(\"dir(st.\" + this +\")\")) and (\"rvs\" in eval(\"dir(st.\" + this +\")\")):\n",
    "                all_distributions.append(this)\n",
    "    else:\n",
    "        for this in ['beta', 'chi2', 'erlang', 'expon', 'gamma', 'logistic',\n",
    "                     'lognorm', 'norm', 'triang', 'truncnorm', 'uniform']:\n",
    "            if (\"fit\" in eval(\"dir(st.\" + this +\")\")) and (\"rvs\" in eval(\"dir(st.\" + this +\")\")):\n",
    "                all_distributions.append(this)\n",
    "\n",
    "    # Estimate distribution parameters from data\n",
    "    for dist_string in all_distributions:\n",
    "        distribution = eval(\"st.\" + dist_string)\n",
    "        \n",
    "        sse = None\n",
    "        args = None\n",
    "        dist = None\n",
    "        \n",
    "        # Try to fit the distribution\n",
    "        try:\n",
    "                      \n",
    "            # Ignore warnings from data that can't be fit\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore')\n",
    "\n",
    "                # fit dist to data\n",
    "                args = distribution.fit(data)\n",
    "                dist = distribution.freeze(*args)\n",
    "\n",
    "                # Calculate fitted PDF and error with fit in distribution\n",
    "                pdf = distribution.pdf(x, *args)\n",
    "                sse = np.sum(np.power(y - pdf, 2.0))\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            # Update lists\n",
    "            dist_list.append(dist_string)\n",
    "            sse_list.append(sse)\n",
    "            args_list.append(args)\n",
    "            dist_gen_list.append(dist)\n",
    "            \n",
    "    \n",
    "    # Use the lists to construct a dictionary object        \n",
    "    fit_comparison = zip(sse_list, dist_list, args_list, dist_gen_list)\n",
    "    fit_comparison = sorted(fit_comparison)\n",
    "    sse_list, dist_list, args_list, dist_gen_list = map(list, zip(*fit_comparison))\n",
    "    \n",
    "    \n",
    "    fit_comparison = {index: {'Name': name, \n",
    "                              'Generator': gen, \n",
    "                              'SSE': sse, \n",
    "                              'Args': args} \\\n",
    "                      for index, (name, gen, sse, args) in enumerate(zip(dist_list, \n",
    "                                                                         dist_gen_list, \n",
    "                                                                         sse_list, \n",
    "                                                                         args_list))}\n",
    "\n",
    "    return fit_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block executes the function for our data. The function returns a dictionary with the fit information, which is stored in the `fit_data` object.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_data = fit_distributions(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block prits the `fit_data` as a pandas `DataFrame` object for easier viewing. \n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(fit_data, orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the function returns the fits in increasing order of sum-of-squares error (SSE). Thus, the first element (index 0) will always reference the fitted distribution with the lowest SSE. We can select the distribution that corresponds to the lowest sum-of-squares error (SSE) as shown in the next code block.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A {fit_data[0]['Name']} distribution provides the lowest SSE of {fit_data[0]['SSE']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Generator` objects that are stored in the dictionary are `scipy` generators that allow us to directly sample from the associated distribution. The following code block shows how we can use the generator object associated with the lowest SSE fit to generate a sample of 10000 data points. This sample is plotted along with the original data.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample = fit_data[0]['Generator'].rvs(10000)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12, 6))\n",
    "\n",
    "ax.hist(data, \n",
    "        density = True, \n",
    "        bins = np.arange(0, 7, 0.25), \n",
    "        edgecolor = 'k', \n",
    "        color = 'b', \n",
    "        label = 'Original Data')\n",
    "ax.hist(my_sample, \n",
    "        density = True, \n",
    "        bins = np.arange(0, 7, 0.25), \n",
    "        alpha = 0.7, \n",
    "        edgecolor = 'k', \n",
    "        color = 'g', \n",
    "        label = 'Sampled Data')\n",
    "ax.set_xlabel('Value', fontsize = 15)\n",
    "ax.set_ylabel('Frequency', fontsize = 15)\n",
    "ax.legend()\n",
    "ax.set_title('Histogram of Data', fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Parametric Distribution Fitting\n",
    "<a id=\"nonParametric_Distribution_Fitting\"> </a>\n",
    "\n",
    "We will now look at a non-parametric technique for approximating a a probability density function for a finite set of sample data that is known as kernel density estimation. As mentioned earlier, kernel density estimation works by centering a fixed *kernel function* at each data point that composes a data set. Summing the kernal functions for all values in the sample space, and then normalizing the resulting sums results in a estimate of the probability distribution function for the data. The smoothness of the kernel density estimate (KDE) is controlled by the kernel function.\n",
    "\n",
    "We will demonstrate kernel density estimation using another set of randomly generated data that is multi-modal, i.e., it has multiple peaks. The following code block generates and plots the data set.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data = np.random.normal(loc = 10, scale = 3, size = 300)\n",
    "data = np.append(data, np.random.normal(loc = 20, scale = 1, size = 50))\n",
    "np.random.shuffle(data)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12, 6))\n",
    "\n",
    "ax.hist(data, \n",
    "        edgecolor = 'k',\n",
    "        bins = np.arange(np.floor(data.min()), np.ceil(data.max()), 1),\n",
    "        label = 'Actual Data')\n",
    "ax.set_ylabel('Frequency', fontsize = 20)\n",
    "ax.set_xlabel('Value', fontsize = 20)\n",
    "ax.set_title('Histogram of Data',  fontsize = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, we define a convenience function that we will use throughout the remainder of this notebook. The function finds the minimum and maximum difference between any two consecutive values in a data set.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_max_diff(data):\n",
    "    '''\n",
    "    This function finds the minimum and maximum difference between any\n",
    "    two consecutive values in a data set\n",
    "    \n",
    "    Arguments\n",
    "    data:\n",
    "        a 1-dimensional list or Numpy array that includes the data\n",
    "        \n",
    "    Returns\n",
    "    min_val, max_val: \n",
    "        a tuple specifiying the minimum and maximum difference between any\n",
    "        two consecutive values in a data set\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    data_copy = data.copy()\n",
    "    data_copy = np.unique(data_copy)\n",
    "    data_copy.sort()\n",
    "    min_val = np.min(np.roll(data_copy, -1)[:-1] - data_copy[:-1])\n",
    "    max_val =  np.max(np.roll(data_copy, -1)[:-1] - data_copy[:-1])\n",
    "    \n",
    "    return min_val, max_val "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block demonstrates the use of the previously defined function.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_min_max_diff(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To motivate the intuition behind kernel density estimation, let's recall how histograms summarize a dataset. In particular, note how the number of bins we use affects our perception of the distribution for the underlying data. When a smaller number of wide bins are used, we obtain a *smoother* approximation for the distribution than we do if we use a use a larger number of narrower bins. This is demonstrated in the following code block that allows a user to vary the number of bins used to construct a histogram for the data.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "@interact(bins=[1, 2, 3, 5, 10, 15, 20, 30, 40, 50, 100, 200, len(data)])\n",
    "\n",
    "def interactive_plot(bins=1):\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (12, 6))\n",
    "\n",
    "    ax.hist(data,  \n",
    "            bins = bins, \n",
    "            edgecolor = 'k', \n",
    "            label = \"Original Data\")\n",
    "    ax.set_xlabel('Value', fontsize = 15)\n",
    "    ax.set_ylabel('Frequency', fontsize = 15)\n",
    "    ax.legend()\n",
    "    ax.set_title('Histogram of Data', fontsize = 15)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of dividng the range of possible values into bins, suppose that we were to center a *Normal* (or *Gaussian*) distribution on each one of the data points in our sample, each with a standard deviation equal to 1.00. The following code block shows what would happen as we vary the number of points that we plot such curves on.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(points=[1, 2, 3, 5, 10, 20, 30, 100, len(data)])\n",
    "\n",
    "def interactive_plot(points=1):\n",
    "\n",
    "    X_plot = np.linspace(np.floor(np.min(data))*0.75, \n",
    "                         np.ceil(np.max(data))*1.1, \n",
    "                         20*int(np.ceil(np.max(data)))).reshape(-1,1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12,6))\n",
    "\n",
    "    samples = []\n",
    "    sigma = 1.0\n",
    "    for i in range(points):\n",
    "        mu = data[i]\n",
    "        new_sample = 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (X_plot - mu)**2 / (2 * sigma**2) )\n",
    "        ax.plot(X_plot, new_sample)\n",
    "        ax.set_xlabel(\"Value\",fontsize = 16)\n",
    "        ax.xaxis.set_tick_params(labelsize=16)\n",
    "        ax.yaxis.set_tick_params(labelsize=16)\n",
    "        ax.set_title(f'{points} Point(s) Shown',fontsize = 20)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of just plotting the individual distributions, what if we summed up the height of distributions overlapping for each value within the range of values.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(points=[1, 2, 3, 5, 10, 20, 30, 100, len(data)])\n",
    "\n",
    "def interactive_plot(points=1):\n",
    "\n",
    "    X_plot = np.linspace(np.floor(np.min(data))*0.75, \n",
    "                         np.ceil(np.max(data))*1.1, \n",
    "                         20*int(np.ceil(np.max(data)))).reshape(-1,1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12,6))\n",
    "\n",
    "    samples = []\n",
    "    sigma = 1.0\n",
    "    for i in range(points):\n",
    "        mu = data[i]\n",
    "        new_sample = 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (X_plot - mu)**2 / (2 * sigma**2) )\n",
    "        samples.append(new_sample)\n",
    "        ax.plot(X_plot, new_sample, linestyle='--', color='b')\n",
    "        ax.set_xlabel(\"Value\",fontsize = 16)\n",
    "        ax.xaxis.set_tick_params(labelsize=16)\n",
    "        ax.yaxis.set_tick_params(labelsize=16)\n",
    "        ax.set_title(f'{points} Point(s) Shown',fontsize = 20)\n",
    "    \n",
    "    samples = np.array(samples)\n",
    "    samples = samples.sum(axis = 0)\n",
    "    ax.plot(X_plot, samples, color = 'k', linewidth=4)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we normalize this sum, we obtain an approximation for the relative frequency of the underlying data.\n",
    "\n",
    "The following code block computes and plots the resulting approximation, which is a kernel density estimate (KDE) for the data that is based on a *gaussian* kernel with a *bandwidth* of 1.0. \n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = np.linspace(np.floor(np.min(data)), \n",
    "                     np.ceil(np.max(data)), \n",
    "                     20*int(np.ceil(np.max(data)))).reshape(-1,1)\n",
    "\n",
    "samples = []\n",
    "sigma = 1.0\n",
    "for i in range(len(data)):\n",
    "    mu = data[i]\n",
    "    new_sample = 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (X_plot - mu)**2 / (2 * sigma**2) )\n",
    "    samples.append(new_sample)\n",
    "    \n",
    "samples = np.array(samples)\n",
    "samples = samples.sum(axis = 0)\n",
    "samples = samples/samples.sum()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "ax.plot(X_plot, samples)\n",
    "ax.set_xlabel('Value',fontsize = 16)\n",
    "ax.set_ylabel('Relative Frequency',fontsize = 16)\n",
    "ax.set_title(f'Gaussian KDE with Bandwidth = {sigma}', fontsize = 20)\n",
    "ax.xaxis.set_tick_params(labelsize=16)\n",
    "ax.yaxis.set_tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the previous approximation bears some resemblance to our original data, it is clearly undersmoothed. The following code block shows how we can increase the smoothing by changing the standard deviation for the underlying gaussian curves that we plot at each data point. This is referred to as varying the *bandwidth* for the KDE.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(bandwidth=[0.1, 0.25, 0.5, 0.75, 1.00, 2.0, 3.0, 5.0, 10.0, 20.0, 100.0])\n",
    "\n",
    "def interactive_plot(bandwidth = 0.1):\n",
    "\n",
    "    X_plot = np.linspace(np.floor(np.min(data))*0.75, np.ceil(np.max(data))*1.1, 20*int(np.ceil(np.max(data)))).reshape(-1,1)\n",
    "\n",
    "    samples = []\n",
    "    sigma = bandwidth\n",
    "    for i in range(len(data)):\n",
    "        mu = data[i]\n",
    "        new_sample = 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (X_plot - mu)**2 / (2 * sigma**2) )\n",
    "        samples.append(new_sample)\n",
    "\n",
    "    samples = np.array(samples)\n",
    "    samples = samples.sum(axis = 0)\n",
    "    samples = samples/samples.sum()\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    ax.plot(X_plot, samples)\n",
    "    ax.set_xlabel('Value',fontsize = 16)\n",
    "    ax.set_ylabel('Relative Frequency',fontsize = 16)\n",
    "    ax.set_title(f'Gaussian KDE with Bandwidth = {sigma}', fontsize = 20)\n",
    "    ax.xaxis.set_tick_params(labelsize=16)\n",
    "    ax.yaxis.set_tick_params(labelsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the intuition behind kernel density estimation, we will now use the functionality offered in the *scikit-learn* package to automate the process. The following code block defines a function that uses *scikit-learn* to determine the best kernel type and bandwidth for approximating the distribution of a data set using a KDE.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_best_kde(data, steps = 25, rtol = 0.1, cv = 3, fit_sample_size = None):\n",
    "    '''\n",
    "    This function determines a best fitting kernel density estimate\n",
    "    using scikit-learn's sklearn.neighbors.KernelDensity method along \n",
    "    scikit-learn's sklearn.model_selection.GridSearchCV method. In \n",
    "    particular, the GridSearchCV method is used to try all possible\n",
    "    kernel types with 100 evenly spaced bandwidths between the minimum\n",
    "    and maximum differences between values in the provided data.\n",
    "    \n",
    "    Arguments:\n",
    "    data: a 1-dimensional list or Numpy array that includes the data\n",
    "    \n",
    "    rtol: the relative tolerance passed to sklearn.neighbors.KernelDensity \n",
    "    method. Higher values offer faster computational times at the cost of\n",
    "    accuracy.\n",
    "    \n",
    "    cv: the number of cross-validation splits the sklearn.model_selection.GridSearchCV \n",
    "    method uses to identify the best kde.\n",
    "    \n",
    "    fit_sample_size: a value that, if specified, denotes that a random sample\n",
    "    of size sample_size should be used to fit the kernel density estimate. This\n",
    "    functionality is added to reduce the high computational times that may\n",
    "    occur when the provided data is large.\n",
    "    \n",
    "    Returns:\n",
    "    data: a dictionary specifes the best bandwidth and kernel.         \n",
    "        \n",
    "        \n",
    "    '''\n",
    "    import sklearn.neighbors as skneighbor\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    import warnings\n",
    "    import numpy as np\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore')\n",
    "        \n",
    "        if fit_sample_size is not None:\n",
    "            data = np.random.choice(data.ravel(), size = fit_sample_size, replace = False)\n",
    "\n",
    "        min_val, max_val = find_min_max_diff(data)\n",
    "\n",
    "        valid_kernels = ['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine']\n",
    "        params = {'bandwidth': np.linspace(min_val, max_val, steps),\n",
    "                 'kernel': valid_kernels}\n",
    "        grid = GridSearchCV(skneighbor.KernelDensity(rtol = rtol), params, cv = cv)\n",
    "        grid.fit(data.reshape(-1, 1))\n",
    "\n",
    "        return grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block executes the function on our data set and stores the returned data in an object named `best_params`.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = fit_best_kde(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block prints the `best_params` object.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function that accepts the following arguments:\n",
    "1. a data set,\n",
    "2. a minimum value (by default, no value is specified),\n",
    "3. a maximum value (by default, no value is specified),\n",
    "4. a bandwidth value (defaults to 1.0),\n",
    "5. a kernel type (defaults to gaussian).\n",
    "\n",
    "It uses these arguments to generate a probability density function for the specified KDE.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_from_kde(data, min_val = 0, max_val = None, bandwidth = 1.0, kernel = 'gaussian'):\n",
    "    '''\n",
    "    This function generates a probability density function (PDF) that is \n",
    "    based on a kernel density estimate that is fit using scikit-learn's\n",
    "    sklearn.neighbors.KernelDensity method. Specifically, it returns two\n",
    "    objects, pdfx and pdfy, that contain the support and probability values\n",
    "    that define the PDF, respectively. \n",
    "    \n",
    "    Arguments:\n",
    "    data: a 1-dimensional list or Numpy array that includes the data\n",
    "    \n",
    "    min_val: the minimum value to include in the PDF support (default\n",
    "    is min_value - 0.10*[range between max_val and min_val values])\n",
    "    \n",
    "    max_val: the maximum value to include in the PDF support (default\n",
    "    is max_value + 0.10*[range between max_val and min_val values])\n",
    "    \n",
    "    bandwidth: the bandwidth for the kernel density estimate.\n",
    "    \n",
    "    cv: the kernel type, which is passed directly to scikit-learn's\n",
    "    sklearn.neighbors.KernelDensity method\n",
    "    \n",
    "    Returns:\n",
    "    data: a dictionary with two keys, x and y. The values are NumPy arrays for the \n",
    "    support (x) and probability values (y) that define the PDF.\n",
    "        \n",
    "        \n",
    "    '''\n",
    "\n",
    "    import sklearn.neighbors as skneighbor\n",
    "    import numpy as np\n",
    "    \n",
    "    data = np.array(data)\n",
    "        \n",
    "    if min_val is None:\n",
    "        min_val = data.min() - 0.10*(data.max() - data.min())\n",
    "        \n",
    "    if max_val is None:\n",
    "        max_val = data.max() + 0.10*(data.max() - data.min())\n",
    "\n",
    "    pdfx = np.linspace(min_val, max_val, 1000)\n",
    "    pdfy = np.exp(skneighbor.KernelDensity(bandwidth = bandwidth, \n",
    "                                           kernel= kernel,\n",
    "                                           rtol = 0.1).fit(data.reshape(-1,1)).score_samples(pdfx.reshape(-1,1)))\n",
    "    pdfy = pdfy/pdfy.sum()\n",
    "    return {'x': pdfx, 'y': pdfy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block uses `pdf_from_kde` to generate a probability density function based on the best KDE that we identifed using the `fit_best_kde` function. The probability density function is plotted.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdf_from_kde(data, \n",
    "                   bandwidth = best_params['bandwidth'], \n",
    "                   kernel = best_params['kernel'])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "ax.plot(pdf['x'], pdf['y'])\n",
    "ax.set_xlabel('Value',fontsize = 16)\n",
    "ax.set_ylabel('Relative Frequency',fontsize = 16)\n",
    "ax.set_title(f'Probability Density Function from KDE', fontsize = 20)\n",
    "ax.xaxis.set_tick_params(labelsize=16)\n",
    "ax.yaxis.set_tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we may wish to generate a random data sample using the KDE. The following `trunc_kde_sample` function accepts the probability density function information returned by our `pdf_from_kde` object for such purposes. \n",
    "\n",
    "Given the way that kernel density estimation works, it can be the case that the probability density function that results from using the technique is defined for values that are not valid. For example, it may be defined for negative values when this is not possible in reality. The `trunc_kde_sample` allows a user to specify reasonable minimum and maximum values, as well as the sample size, to make sure that the generated sample includes valid values.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_kde_sample(pdfx, pdfy, low = None, high = None, sample_size = 100, seed = 0):\n",
    "    '''\n",
    "    This function requires two array-like data objects, pdfx and pdfy,\n",
    "    that specify the support and probabilities for a probability density\n",
    "    function (PDF) defined by a kernel density estimate (see the \n",
    "    pdf_from_kde function). These data objects are used to generate a sample\n",
    "    from the defined PDF that falls between optional lower and upper bounds.\n",
    "    \n",
    "    Arguments:\n",
    "    pdfx: a 1-dimensional list or Numpy array that specifies the PDF's support\n",
    "    \n",
    "    pdfy: a 1-dimensional list or Numpy array that specifies the probability\n",
    "    for each value in the PDF's support\n",
    "    \n",
    "    low: the lower bound for values in the generated sample\n",
    "    \n",
    "    high: the upper bound for values in the generated sample\n",
    "    \n",
    "    sample_size: the size of the sample to be drawn (1-dimensional only)\n",
    "    \n",
    "    seed: seed for Numpy's random number generator\n",
    "    \n",
    "    Returns:\n",
    "    gen_sample: the generated sample as a Numpy array.\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    import numpy as np\n",
    "    \n",
    "    pdfx = np.array(pdfx)    \n",
    "    pdfy = np.array(pdfy)\n",
    "    \n",
    "    if (low != None) and (high != None):\n",
    "        mask = pdfx >= low\n",
    "        pdfx = pdfx[mask]\n",
    "        pdfy = pdfy[mask]\n",
    "        \n",
    "        mask = pdfx <= high\n",
    "        pdfx = pdfx[mask]\n",
    "        pdfy = pdfy[mask]\n",
    "        \n",
    "    elif (low != None):\n",
    "        mask = pdfx >= low\n",
    "        pdfx = pdfx[mask]\n",
    "        pdfy = pdfy[mask]\n",
    "        \n",
    "    elif (high != None):\n",
    "        mask = pdfx <= high\n",
    "        pdfx = pdfx[mask]\n",
    "        pdfy = pdfy[mask]\n",
    "        \n",
    "    pdfy = pdfy/pdfy.sum()\n",
    "    cdf = pdfy.cumsum()\n",
    "    cdf = cdf - cdf.min()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    random_nums = np.random.rand(sample_size)\n",
    "    gen_sample = []\n",
    "    for i in random_nums:\n",
    "        gen_sample.append(pdfx[(cdf < i).argmin() - 1])\n",
    "    \n",
    "    return np.array(gen_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block demonstrates the use of the `trunc_kde_sample` function.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample = trunc_kde_sample(pdf['x'], pdf['y'], low = 0, sample_size = 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block plots the sampled data along with the original sample.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (12, 6))\n",
    "\n",
    "ax.hist(data, \n",
    "        density = True, \n",
    "        bins = np.arange(np.floor(data.min()), np.ceil(data.max()), 1), \n",
    "        edgecolor = 'k', \n",
    "        color = 'b', \n",
    "        label = \"Original Data\")\n",
    "ax.hist(my_sample, \n",
    "        density = True, \n",
    "        bins = np.arange(np.floor(data.min()), np.ceil(data.max()), 1), \n",
    "        alpha = 0.7, \n",
    "        edgecolor = 'k', \n",
    "        color = 'g', \n",
    "        label = \"Sampled from Fitted Distribution\")\n",
    "ax.set_xlabel('Value', fontsize = 15)\n",
    "ax.set_ylabel('Frequency', fontsize = 15)\n",
    "ax.legend()\n",
    "ax.set_title('Histogram of Data', fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Putting it all together\n",
    "<a id=\"Putting_it_all_together\"> </a>\n",
    "\n",
    "Just as an illustration, we will now use the developed function to fit parametric and non-parametric distributions to a set of data and generate samples with minimal code. The following code block generates a new data set for this demonstration.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data  = np.random.lognormal(mean=1.9, sigma = 0.25, size = 500)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12, 6))\n",
    "\n",
    "ax.hist(data,  \n",
    "        bins = 'auto', \n",
    "        edgecolor = 'k', \n",
    "        label = \"Original Data\")\n",
    "ax.set_xlabel('Value', fontsize = 15)\n",
    "ax.set_ylabel('Frequency', fontsize = 15)\n",
    "ax.legend()\n",
    "ax.set_title('Histogram of Data', fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block uses the parametric approach for fitting a distribution and generating a sample of 1000 data points from the best fitting distribution. The sample is plotted along with the original data.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_data = fit_distributions(data)\n",
    "my_parametric_sample = fit_data[0]['Generator'].rvs(1000)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12, 6))\n",
    "ax.hist(data,  \n",
    "        bins = 'auto', \n",
    "        edgecolor = 'k', \n",
    "        color = 'b',\n",
    "        density = True,\n",
    "        label = 'Original Data')\n",
    "ax.hist(my_parametric_sample,  \n",
    "        bins = 'auto', \n",
    "        edgecolor = 'k',\n",
    "        color = 'g',\n",
    "        density = True,\n",
    "        alpha = 0.7,\n",
    "        label = 'Parametric  Sample')\n",
    "ax.set_xlabel('Value', fontsize = 15)\n",
    "ax.set_ylabel('Frequency', fontsize = 15)\n",
    "ax.legend()\n",
    "ax.set_title('Histogram of Data', fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block uses the non-parametric approach for fitting a distribution and generating a sample of 1000 data points. The sample is plotted along with the original data and the sample generated using parameteric methods.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = fit_best_kde(data)\n",
    "pdf = pdf_from_kde(data, \n",
    "                   bandwidth = best_params['bandwidth'], \n",
    "                   kernel = best_params['kernel'])\n",
    "my_nonparametric_sample = trunc_kde_sample(pdf['x'],\n",
    "                                           pdf['y'], \n",
    "                                           low = 0, \n",
    "                                           sample_size = 1000)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12, 6))\n",
    "ax.hist(data,  \n",
    "        bins = 'auto', \n",
    "        edgecolor = 'k', \n",
    "        color = 'b',\n",
    "        density = True,\n",
    "        label = 'Original Data')\n",
    "ax.hist(my_parametric_sample,  \n",
    "        bins = 'auto', \n",
    "        edgecolor = 'k',\n",
    "        color = 'g',\n",
    "        density = True,\n",
    "        alpha = 0.7,\n",
    "        label = 'Parametric  Sample')\n",
    "ax.hist(my_nonparametric_sample,  \n",
    "        bins = 'auto', \n",
    "        edgecolor = 'k',\n",
    "        color = 'y',\n",
    "        density = True,\n",
    "        alpha = 0.3,\n",
    "        label = 'Non-Parametric  Sample')\n",
    "ax.set_xlabel('Value', fontsize = 15)\n",
    "ax.set_ylabel('Frequency', fontsize = 15)\n",
    "ax.legend()\n",
    "ax.set_title('Histogram of Data', fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, both approaches are able to provide reasonable approximations to the data.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application - Estimating Manufacturing Lead Time\n",
    "\n",
    "The `distfit` modules of the `OM527_functions` library contains the functions defined earlier in this noteboook. The following code block imports the library, which should be in the same directory as this notebook, and assigns it to the alias `omf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import OM527_functions as omf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider an example of a manufacturing and distribution process where parts are ordered, value-adding activities are performed, and finished products are shipped to illustrate how we can use the module to fit distributions and perform a Monte Carlo simulation to determine lead time quotes that guarantee a desired service level. More information on Monte Carlo simulation can be found at https://en.wikipedia.org/wiki/Monte_Carlo_method. The following code block generates 400 observations for the part delivery times (`delivery_times`), manufacturing times (`manufacturing_times`), and shipment times (`shipment_times`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 400\n",
    "\n",
    "np.random.seed(0)\n",
    "delivery_times = np.array(list(np.random.normal(loc = 10, scale = 3, size = size//2))\n",
    "                          + list(np.random.normal(loc = 25, scale = 5, size = size//2)))\n",
    "\n",
    "manufacturing_times = np.random.triangular(left = 10, mode = 13, right = 20, size = size)\n",
    "\n",
    "shipment_times = np.random.triangular(left = 10, mode = 13, right = 20, size = size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a simple helper function that we can use to visualize the distribution of an array of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_histogram(observation_data, \n",
    "                       percentiles = None):\n",
    "    '''\n",
    "    Generates a histogram of the provided observation_data. Users may provide\n",
    "    a python list of percentiles to include them in the plot. For example, if\n",
    "    the user provides the list [50, 95], the plot will draw vertical lines indicating\n",
    "    the 50th and 95th percentiles of the data.\n",
    "    '''\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize = (6, 4))\n",
    "\n",
    "    ax.hist(observation_data,\n",
    "            alpha = 0.5,\n",
    "            edgecolor = 'k')\n",
    "    \n",
    "    if not percentiles is None:\n",
    "        for percentile in percentiles:\n",
    "            ax.axvline(np.percentile(observation_data, percentile), color = 'k')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block uses the function to visualize the `delivery_times` observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_histogram(delivery_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block uses the function to visualize the `manufacturing_times` observations. The plot includes a vertical line that indicates the median, i.e., $50^{th}$ percentile of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_histogram(manufacturing_times, percentiles = [50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block uses the function to visualize the `shipment_times` observations. The plot includes vertical lines that indicate the $95^{th}$ and $99^{th}$ percentiles of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_histogram(shipment_times, percentiles = [95, 99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block performs a Monte Carlo simulation of the delivery, manufacturing, and shipment process for a single part using 10,000 samples. This simulation allows us to determine the distribution of the lead time for this process. The distribution is visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10000\n",
    "\n",
    "best_params = omf.distfit.fit_best_kde(delivery_times)\n",
    "pdf = omf.distfit.pdf_from_kde(delivery_times, \n",
    "                               bandwidth = best_params['bandwidth'],\n",
    "                               kernel = best_params['kernel'])\n",
    "delivery_sample = omf.distfit.trunc_kde_sample(pdf['x'],\n",
    "                                               pdf['y'],\n",
    "                                               low = 0, \n",
    "                                               sample_size = samples)\n",
    "\n",
    "fit_data = omf.distfit.fit_distributions(manufacturing_times)\n",
    "manufacturing_sample = fit_data[0]['Generator'].rvs(samples)\n",
    "\n",
    "fit_data = omf.distfit.fit_distributions(shipment_times)\n",
    "shipment_sample = fit_data[0]['Generator'].rvs(samples)\n",
    "\n",
    "LT_sample = delivery_sample + manufacturing_sample + shipment_sample\n",
    "\n",
    "generate_histogram(LT_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how we can determine various percentiles of the lead time distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(LT_sample, q = [50, 60, 70, 80, 90, 95, 99, 99.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
